1、总结决策树模型结构

决策树是一种典型的分类方法，首先对数据进行处理， 利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。

本质上决策树是通过一系列规则对数据进行分类的过程。

决策树的基本组成部分： 决策结点、 分支和叶子。

决策树的决策过程需要从决策树的根节点开始，待测数据与决策树中的特征节点进行比较，并按照比较结果选择选择下一比较分支，直到叶子节点作为最终的决策结果。

有两种树：分类树--对离散变量做决策树，回归树--对连续变量做决策树

决策树的学习过程分为三个部分：

特征选择：从训练数据的特征中选择一个特征作为当前节点的分裂标准（特征选择的标准不同产生了不同的特征决策树算法）。
决策树生成：根据所选特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止声场。
剪枝：决策树容易过拟合，需要剪枝来缩小树的结构和规模（包括预剪枝和后剪枝）。

CLS（Concept Learning System） 

算法步骤：

1）生成一颗空决策树和一张训练样本属性集;

2）若训练样本集T 中所有的样本都属于同一类,则生成结点T , 并终止学习算法;否则

3）根据某种策略从训练样本属性表中选择属性A 作为测试属性, 生成测试结点A

4）若A的取值为v1,v2,…,vm, 则根据A 的取值的不同,将T 划分成m个子集T1,T2,…,Tm;

5）从训练样本属性表中删除属性A;

6）转步骤2, 对每个子集递归调用CLS;


2. 学习信息增益、学习信息增益率

信息增益原则对于每个分支节点，都会乘以其权重，也就是说，由于权重之和为1，所以分支节点分的越多，即每个节点数据越小，纯度可能越高。这样会导致信息熵准则偏爱那些取值数目较多的属性。为了解决该问题，这里引入了信息增益率。
信息增益率原则可能对取值数目较少的属性更加偏爱,为了解决这个问题，可以先找出信息增益在平均值以上的属性，在从中选择信息增益率最高的。

3. 学习ID3算法优缺点

ID3算法是由Ross Quinlan提出的决策树的一种算法实现，以信息论为基础，以信息熵和信息增益为衡量标准，从而实现对数据的归纳分类。
ID3算法是建立在奥卡姆剃刀的基础上：越是小型的决策树越优于大的决策树（be simple简单理论）。
ID3算法可用于划分标准称型数据，但存在一些问题：
1）没有剪枝过程，为了去除过渡数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点;
2）信息增益的方法偏向选择具有大量值的属性，也就是说某个属性特征索取的不同值越多，那么越有可能作为分裂属性，这样是不合理的；
3）只可以处理离散分布的数据特征

学习C4.5算法优缺点：

C4.5算法与ID3算法相似，C4.5 算法对ID3算法进行了改进。C4.5 在生成的过程中，用信息增益比来选择特征。

优点:

通过信息增益率选择分裂属性，克服了ID3算法中通过信息增益倾向于选择拥有多个属性值的属性作为分裂属性的不足；

能够处理离散型和连续型的属性类型，即将连续型的属性进行离散化处理；

构造决策树之后进行剪枝操作；

能够处理具有缺失属性值的训练数据。

缺点: 

C4.5生成的是多叉树，即一个父节点可以有多个节点，因此算法的计算效率较低，特别是针对含有连续属性值的训练样本时表现的尤为突出。

算法在选择分裂属性时没有考虑到条件属性间的相关性，只计算数据集中每一个条件属性与决策属性之间的期望信息，有可能影响到属性选择的正确性。

C4.5算法只适合于能够驻留内存的数据集，当训练集大得无法在内存容纳时，程序无法运行；

改进
1）用信息增益率来选择属性，克服了用信息增益选择属性偏向选择多值属性的不足
2）在构造树的过程中进行剪枝
3）对连续属性进行离散化
4）能够对不完整的数据进行处理

对连续值的处理：
将连续型的属性变量进行离散化处理形成决策树的训练集：
1）将需要处理的样本（对应根节点）或样本子集（对应子树）按照连续变量的大小从小到大进行排序
2）假设该属性对应不同的属性值共N个，那么总共有N-1个可能的候选分割值点，每个候选的分割阈值点的值为上述排序后的属性值中两两前后连续元素的中点
3）用信息增益选择最佳划分
